{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["riv0vZp5ig3t"],"toc_visible":true,"authorship_tag":"ABX9TyObyEyq+halQXKcCcSrNTV7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["La descomposici√≥n en sesgo y varianza es un concepto utilizado en el aprendizaje autom√°tico para analizar el rendimiento de un modelo y comprender las fuentes de error.\n","\n","Sesgo (Bias): El sesgo se refiere a la capacidad de un modelo para realizar predicciones precisas en promedio. Un modelo con un sesgo alto tiende a simplificar demasiado los datos y puede perder detalles importantes, lo que resulta en un rendimiento deficiente en el conjunto de datos de entrenamiento y prueba. Esto se conoce como \"underfitting\". Un modelo con un sesgo bajo, por otro lado, tiene la capacidad de capturar patrones complejos en los datos y puede ajustarse bien tanto al conjunto de entrenamiento como al conjunto de prueba.\n","\n","Varianza (Variance): La varianza se refiere a la sensibilidad de un modelo a las fluctuaciones en los datos de entrenamiento. Un modelo con alta varianza se ajusta demasiado a los datos de entrenamiento y tiene dificultades para generalizar a nuevos datos, lo que resulta en un rendimiento deficiente en el conjunto de prueba. Esto se conoce como \"overfitting\". Por otro lado, un modelo con baja varianza tiene una capacidad limitada para capturar la complejidad de los datos y puede perder patrones importantes, lo que tambi√©n puede llevar a un rendimiento deficiente en el conjunto de prueba.\n","\n","El objetivo es encontrar un equilibrio entre el sesgo y la varianza para obtener un modelo que se ajuste bien a los datos de entrenamiento y generalize bien a nuevos datos. Esto se conoce como el compromiso sesgo-varianza. Al comprender la descomposici√≥n en sesgo y varianza, se puede tomar decisiones m√°s informadas sobre la elecci√≥n de modelos y t√©cnicas de regularizaci√≥n para mejorar el rendimiento predictivo."],"metadata":{"id":"EU_A7Z8peV-H"}},{"cell_type":"markdown","source":["- **Underfitting**: los errores de entrenamiento y testeo son ambos grandes. Sesgo alto.\n","- **Overfitting**: error al entrenar es peque√±o, pero al testear aumenta. Varianza alta.\n","- Si el espacio de hip√≥tesis que se estudia es grande hay mas tendencia a sobreajustar."],"metadata":{"id":"nEpek_Czehe1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PRFEiJ3FNZG9"}},{"cell_type":"code","source":["!pip install -U mlxtend"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":627},"id":"as3DzhoJmqbr","executionInfo":{"status":"ok","timestamp":1686824995226,"user_tz":180,"elapsed":6397,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"b2d0e87e-d202-42e3-fdad-c2719602ca10"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.14.0)\n","Collecting mlxtend\n","  Downloading mlxtend-0.22.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.10.1)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.22.4)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.5.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.2)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.7.1)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend) (67.7.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend) (2022.7.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n","Installing collected packages: mlxtend\n","  Attempting uninstall: mlxtend\n","    Found existing installation: mlxtend 0.14.0\n","    Uninstalling mlxtend-0.14.0:\n","      Successfully uninstalled mlxtend-0.14.0\n","Successfully installed mlxtend-0.22.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["mlxtend"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["### problema 2  Votacion de la mayoria"],"metadata":{"id":"ElrzgK_VnBXD"}},{"cell_type":"markdown","source":["`EnsembleVoteClassifier` es un clasificador de votaci√≥n de conjunto implementado en la biblioteca `mlxtend`. Combina m√∫ltiples clasificadores base para realizar predicciones mediante votaci√≥n.\n","\n","Cuando se crea un objeto `EnsembleVoteClassifier`, se deben proporcionar dos argumentos principales:\n","- `clfs`: una lista de clasificadores base que se utilizar√°n en el conjunto de votaci√≥n.\n","- `weights`: una lista de pesos asignados a cada clasificador base, que determina la importancia relativa de cada clasificador en la votaci√≥n.\n","\n","Durante la fase de entrenamiento, cada clasificador base se ajusta (entrena) con los datos de entrenamiento proporcionados. Durante la fase de predicci√≥n, los clasificadores base hacen sus predicciones individualmente y luego se realiza una votaci√≥n para determinar la clase final de la muestra.\n","\n","El tipo de votaci√≥n se define mediante el par√°metro `voting` en la creaci√≥n del objeto `EnsembleVoteClassifier`. Puede ser \"hard\" (votaci√≥n dura) o \"soft\" (votaci√≥n suave):\n","- En la votaci√≥n dura, la clase m√°s com√∫n predicha por los clasificadores base se elige como la predicci√≥n final.\n","- En la votaci√≥n suave, se asigna una puntuaci√≥n de confianza a cada clase para cada clasificador base, y las predicciones se ponderan por estas puntuaciones antes de realizar la votaci√≥n final.\n","\n","Al utilizar `EnsembleVoteClassifier`, se busca aprovechar las fortalezas de diferentes clasificadores base y mejorar el rendimiento general del conjunto a trav√©s de la votaci√≥n. Puede ser especialmente √∫til cuando los clasificadores base tienen enfoques y caracter√≠sticas diferentes.\n","\n","En el c√≥digo que proporcionaste, `EnsembleVoteClassifier` se utiliza para combinar tres clasificadores de √°rbol de decisi√≥n con diferentes profundidades m√°ximas en un clasificador de votaci√≥n de conjunto, donde cada clasificador base tiene un peso igual (1) en la votaci√≥n."],"metadata":{"id":"VC1SBz0Unvvz"}},{"cell_type":"markdown","source":["#####explicacion del codigo"],"metadata":{"id":"h4JgPJmmnxQr"}},{"cell_type":"markdown","source":["\n","Este c√≥digo realiza un ejemplo de clasificaci√≥n utilizando el conjunto de datos Iris. Aqu√≠ est√° lo que hace cada parte del c√≥digo:\n","\n","Importaci√≥n de las bibliotecas necesarias:\n","\n","model_selection de sklearn para realizar selecci√≥n de modelos y evaluaci√≥n.\n","DecisionTreeClassifier de sklearn.tree para crear un clasificador de √°rbol de decisi√≥n.\n","train_test_split de sklearn.model_selection para dividir los datos en conjuntos de entrenamiento y prueba.\n","datasets de sklearn para cargar el conjunto de datos Iris.\n","EnsembleVoteClassifier de mlxtend.classifier para crear un clasificador de votaci√≥n de conjunto.\n","Carga del conjunto de datos Iris:\n","\n","Utiliza la funci√≥n load_iris() de datasets para cargar el conjunto de datos Iris.\n","Divide los datos en caracter√≠sticas (X) y etiquetas (y), tomando las columnas 0 y 3 del conjunto de datos original.\n","Divisi√≥n de los datos en conjuntos de entrenamiento, validaci√≥n y prueba:\n","\n","Utiliza la funci√≥n train_test_split para dividir los datos en conjuntos de entrenamiento y prueba, con un tama√±o de prueba del 25% y una semilla aleatoria de 1.\n","A partir del conjunto de entrenamiento, se vuelve a dividir en conjuntos de entrenamiento y validaci√≥n, con un tama√±o de validaci√≥n del 25% y una semilla aleatoria de 1.\n","Creaci√≥n de clasificadores:\n","\n","Se definen tres clasificadores de √°rbol de decisi√≥n con diferentes profundidades m√°ximas (1, 3 y la predeterminada).\n","Se crea un clasificador de votaci√≥n de conjunto (EnsembleVoteClassifier) que combina los tres clasificadores anteriores con pesos iguales.\n","Entrenamiento y evaluaci√≥n de los clasificadores:\n","\n","Se itera sobre los clasificadores y se realiza el entrenamiento en el conjunto de entrenamiento.\n","Se eval√∫a el rendimiento de cada clasificador utilizando el conjunto de validaci√≥n y se imprime la precisi√≥n de la clasificaci√≥n.\n","Evaluaci√≥n del clasificador de votaci√≥n de conjunto:\n","\n","Se eval√∫a el rendimiento del clasificador de votaci√≥n de conjunto utilizando el conjunto de prueba y se imprime la precisi√≥n de la clasificaci√≥n.\n","En resumen, este c√≥digo muestra c√≥mo utilizar los clasificadores de √°rbol de decisi√≥n y el clasificador de votaci√≥n de conjunto en el conjunto de datos Iris, realizando la evaluaci√≥n tanto en un conjunto de validaci√≥n como en un conjunto de prueba."],"metadata":{"id":"VPRoxAGZnIj_"}},{"cell_type":"markdown","source":["####Codigo"],"metadata":{"id":"3eUULJ4Fn2EL"}},{"cell_type":"code","source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from mlxtend.classifier import EnsembleVoteClassifier\n","\n","iris = datasets.load_iris()\n","X, y = iris.data[:, [0, 3]], iris.target\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","clf1 = DecisionTreeClassifier(random_state=1)\n","clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n","clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n","eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n","\n","labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n","for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n","\n","    clf.fit(X_train, y_train)\n","    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n","\n","print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_WW7IgbmW8r","executionInfo":{"status":"ok","timestamp":1686825018741,"user_tz":180,"elapsed":4799,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"8aeb3716-04e8-4e3c-afc1-289e510cb30b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 84 28 38\n","Validation Accuracy: 0.86 [Classifier 1]\n","Validation Accuracy: 0.82 [Classifier 2]\n","Validation Accuracy: 0.93 [Classifier 3]\n","Validation Accuracy: 0.93 [Ensemble]\n","Test Accuracy: 0.95\n"]}]},{"cell_type":"markdown","source":["#### load breast cancer"],"metadata":{"id":"BOYfAfV8r8DJ"}},{"cell_type":"code","source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from mlxtend.classifier import EnsembleVoteClassifier\n","\n","# Cargar el conjunto de datos de c√°ncer de mama\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","# Dividir los datos de entrenamiento en conjuntos de entrenamiento y validaci√≥n\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","# Crear los clasificadores base\n","clf1 = DecisionTreeClassifier(random_state=1)\n","clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n","clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n","\n","# Crear el clasificador de conjunto (Ensemble)\n","eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n","\n","labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n","for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n","    clf.fit(X_train, y_train)\n","    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n","\n","print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42DVVfUxsDbS","executionInfo":{"status":"ok","timestamp":1686826401726,"user_tz":180,"elapsed":361,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"5d320f7a-31d2-4c91-e38d-70f652fd378f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 319 107 143\n","Validation Accuracy: 0.93 [Classifier 1]\n","Validation Accuracy: 0.92 [Classifier 2]\n","Validation Accuracy: 0.92 [Classifier 3]\n","Validation Accuracy: 0.92 [Ensemble]\n","Test Accuracy: 0.91\n"]}]},{"cell_type":"markdown","source":["### problema 3 Bagging: Bootstrap Aggregating\n","\n","\n"],"metadata":{"id":"PP6C_Eu9oPtk"}},{"cell_type":"markdown","source":["El BaggingClassifier es otro m√©todo de conjunto (ensemble) implementado en la biblioteca scikit-learn. Bagging significa \"ensacado\" y es una t√©cnica que se utiliza para mejorar la estabilidad y precisi√≥n de los modelos de aprendizaje autom√°tico.\n","\n","En t√©rminos simples, el BaggingClassifier crea m√∫ltiples instancias de un clasificador base y entrena cada instancia en una muestra aleatoria con reemplazo del conjunto de datos de entrenamiento original. Luego, combina las predicciones de cada clasificador base mediante votaci√≥n (promediando en caso de regresi√≥n) para obtener una predicci√≥n final.\n","\n","Al crear un objeto BaggingClassifier, se deben proporcionar los siguientes argumentos principales:\n","\n","base_estimator: el clasificador base que se utilizar√° en cada instancia.\n","n_estimators: el n√∫mero de clasificadores base (instancias) que se crear√°n.\n","max_samples: el n√∫mero de muestras seleccionadas aleatoriamente con reemplazo para entrenar cada clasificador base.\n","max_features: el n√∫mero de caracter√≠sticas seleccionadas aleatoriamente en cada divisi√≥n de los datos durante la construcci√≥n del clasificador base.\n","Adem√°s, el BaggingClassifier ofrece opciones adicionales, como el uso de muestras ponderadas, paralelizaci√≥n para entrenamiento r√°pido y la opci√≥n de permitir o no muestras fuera de la bolsa para estimaciones.\n","\n","El objetivo principal del BaggingClassifier es reducir la varianza y el sobreajuste al promediar las predicciones de m√∫ltiples clasificadores base entrenados en muestras aleatorias. Esto puede mejorar el rendimiento general del modelo y hacerlo m√°s robusto.\n","\n","En resumen, el BaggingClassifier crea un conjunto de clasificadores base, los entrena en muestras aleatorias y combina sus predicciones mediante votaci√≥n. Esto ayuda a mejorar la estabilidad y precisi√≥n del modelo al reducir la varianza y el sobreajuste."],"metadata":{"id":"wDNQByfxpRgr"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n","\n","\n","Este c√≥digo utiliza el conjunto de datos Breast Cancer para realizar la clasificaci√≥n mediante el uso de un clasificador de √°rbol de decisi√≥n y un clasificador Bagging.\n","\n","Aqu√≠ est√° lo que hace cada parte del c√≥digo:\n","\n","Importaci√≥n de las bibliotecas necesarias:\n","\n","model_selection de sklearn para realizar selecci√≥n de modelos y evaluaci√≥n.\n","DecisionTreeClassifier de sklearn.tree para crear un clasificador de √°rbol de decisi√≥n.\n","train_test_split de sklearn.model_selection para dividir los datos en conjuntos de entrenamiento y prueba.\n","datasets de sklearn para cargar conjuntos de datos.\n","BaggingClassifier de sklearn.ensemble para crear un clasificador Bagging.\n","Carga del conjunto de datos Breast Cancer:\n","\n","Utiliza la funci√≥n load_breast_cancer() de datasets para cargar el conjunto de datos Breast Cancer.\n","Divide los datos en caracter√≠sticas (X) y etiquetas (y).\n","Divisi√≥n de los datos en conjuntos de entrenamiento, validaci√≥n y prueba:\n","\n","Utiliza la funci√≥n train_test_split para dividir los datos en conjuntos de entrenamiento y prueba, con un tama√±o de prueba del 25% y una semilla aleatoria de 1.\n","A partir del conjunto de entrenamiento, se vuelve a dividir en conjuntos de entrenamiento y validaci√≥n, con un tama√±o de validaci√≥n del 25% y una semilla aleatoria de 1.\n","Creaci√≥n y entrenamiento de un clasificador de √°rbol de decisi√≥n:\n","\n","Se crea un clasificador de √°rbol de decisi√≥n (DecisionTreeClassifier) con criterio de entrop√≠a y una semilla aleatoria de 1.\n","Se asigna el clasificador de √°rbol de decisi√≥n a la variable clf.\n","Se entrena el clasificador de √°rbol de decisi√≥n utilizando los datos de entrenamiento (X_train y y_train).\n","Creaci√≥n y entrenamiento de un clasificador Bagging:\n","\n","Se crea un clasificador Bagging (BaggingClassifier) utilizando el clasificador de √°rbol de decisi√≥n como estimador base.\n","Se configuran varios par√°metros, como el n√∫mero de estimadores en el ensemble (n_estimators), el uso del c√°lculo del puntaje fuera de la bolsa (oob_score), el muestreo con reemplazo (bootstrap), etc.\n","Se entrena el clasificador Bagging utilizando los datos de entrenamiento (X_train y y_train).\n","Evaluaci√≥n de los clasificadores:\n","\n","Se imprime la precisi√≥n del clasificador de √°rbol de decisi√≥n utilizando los datos de prueba (X_test y y_test).\n","Se imprime el puntaje fuera de la bolsa del clasificador Bagging (bag.oob_score_).\n","Se imprime la precisi√≥n del clasificador Bagging utilizando los datos de prueba (X_test y y_test).\n","En resumen, este c√≥digo carga el conjunto de datos Breast Cancer, entrena un clasificador de √°rbol de decisi√≥n y un clasificador Bagging, y luego eval√∫a el rendimiento de ambos clasificadores en el conjunto de prueba."],"metadata":{"id":"_wNKXPORo85D"}},{"cell_type":"markdown","source":["####codigo"],"metadata":{"id":"NYS8bpOEo7Oa"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYwrw6HW_B90","executionInfo":{"status":"ok","timestamp":1686823714717,"user_tz":180,"elapsed":4525,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"a6cb4a64-9f86-485e-f6b1-bd5b1d3f105a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 319 107 143\n","tree Accuracy: 0.94\n","OOB Accuracy: 0.95\n","Test Accuracy: 0.97\n"]}],"source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from sklearn.ensemble import BaggingClassifier\n","\n","from sklearn.datasets import load_breast_cancer\n","cancer = load_breast_cancer()\n","X = cancer.data\n","y = cancer.target\n","target_names = cancer.target_names\n","\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","tree = DecisionTreeClassifier(criterion='entropy',\n","                              random_state=1,\n","                              max_depth=None)\n","\n","clf = tree\n","clf.fit(X_train, y_train)\n","\n","bag = BaggingClassifier(\n","    estimator=tree,                    # Clasificador base utilizado para construir el ensemble\n","    n_estimators=500,                  # N√∫mero de estimadores en el ensemble\n","    oob_score=True,                     # Indicador para calcular el score fuera de la bolsa (OOB score)\n","    bootstrap=True,                     # Indicador para realizar muestreo con reemplazo (bootstrap)\n","    bootstrap_features=False,           # Indicador para realizar muestreo de caracter√≠sticas con reemplazo\n","    n_jobs=1,                           # N√∫mero de trabajos en paralelo para ajustar los estimadores base\n","    random_state=42                     # Semilla utilizada por el generador de n√∫meros aleatorios para reproducibilidad\n",")\n","\n","bag.fit(X_train, y_train)\n","\n","print('tree Accuracy: %0.2f' % clf.score(X_test, y_test))\n","print('OOB Accuracy: %0.2f' % bag.oob_score_)\n","print('Test Accuracy: %0.2f' % bag.score(X_test,y_test))"]},{"cell_type":"markdown","source":["### problema 4 adaptative boosting"],"metadata":{"id":"riv0vZp5ig3t"}},{"cell_type":"markdown","source":["El `AdaBoostClassifier` es un algoritmo de conjunto (ensemble) de clasificaci√≥n implementado en la biblioteca `scikit-learn`. El nombre \"AdaBoost\" proviene de \"Adaptive Boosting\" (\"impulso adaptable\"), que describe su enfoque de mejorar iterativamente el rendimiento del clasificador.\n","\n","El algoritmo `AdaBoostClassifier` combina m√∫ltiples clasificadores d√©biles (tambi√©n conocidos como estimadores base) en un clasificador fuerte. Cada clasificador d√©bil se entrena en un conjunto de datos ponderado, donde las muestras mal clasificadas por los clasificadores anteriores se les da m√°s peso para el siguiente clasificador. En resumen, el algoritmo se adapta y se centra en las muestras dif√≠ciles de clasificar, mejorando as√≠ el rendimiento general.\n","\n","A continuaci√≥n, se describen los par√°metros principales del `AdaBoostClassifier`:\n","\n","- `base_estimator`: especifica el clasificador d√©bil utilizado en el ensemble. Puede ser cualquier clasificador de `scikit-learn` compatible con clasificaci√≥n binaria, como `DecisionTreeClassifier`. De forma predeterminada, se utiliza un √°rbol de decisi√≥n de profundidad 1 (`DecisionTreeClassifier(max_depth=1)`).\n","\n","- `n_estimators`: especifica el n√∫mero de clasificadores d√©biles (estimadores) en el ensemble. Cuanto mayor sea este n√∫mero, m√°s complejo ser√° el ensemble. Sin embargo, tambi√©n puede aumentar el riesgo de sobreajuste. El valor predeterminado es `50`.\n","\n","- `learning_rate`: controla la contribuci√≥n de cada clasificador d√©bil en el ensemble. Un valor m√°s peque√±o disminuye la importancia de cada clasificador, lo que puede ayudar a prevenir el sobreajuste. El valor predeterminado es `1.0`.\n","\n","- `algorithm`: especifica el algoritmo utilizado para el c√°lculo de los pesos de muestra. Puede ser \"SAMME\" (Stagewise Additive Modeling using a Multiclass Exponential loss function) o \"SAMME.R\" (SAMME con la tasa de probabilidad). \"SAMME.R\" generalmente se recomienda ya que puede manejar estimadores con valores de probabilidad continuos. El valor predeterminado es \"SAMME.R\".\n","\n","- `random_state`: establece la semilla para la generaci√≥n de n√∫meros aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `...`: hay otros par√°metros adicionales que se pueden configurar, como `max_depth`, `min_samples_split`, etc., que afectan al clasificador d√©bil utilizado.\n","\n","En resumen, el `AdaBoostClassifier` utiliza clasificadores d√©biles para formar un ensemble y mejorar iterativamente su rendimiento al centrarse en las muestras dif√≠ciles de clasificar. Los par√°metros mencionados anteriormente permiten ajustar la complejidad del ensemble y controlar su capacidad de generalizaci√≥n."],"metadata":{"id":"B1CDTvoQpqLK"}},{"cell_type":"markdown","source":["#### explicacion del codigo"],"metadata":{"id":"ZTmFsS2WpsBb"}},{"cell_type":"markdown","source":["Este c√≥digo utiliza el algoritmo AdaBoostClassifier para realizar la clasificaci√≥n mediante el uso de un clasificador de √°rbol de decisi√≥n como estimador base.\n","\n","Aqu√≠ est√° lo que hace cada parte del c√≥digo:\n","\n","1. Importaci√≥n de las bibliotecas necesarias:\n","   - `AdaBoostClassifier` de `sklearn.ensemble` para crear un clasificador AdaBoost.\n","   - `DecisionTreeClassifier` de `sklearn.tree` para crear un clasificador de √°rbol de decisi√≥n.\n","\n","2. Creaci√≥n y entrenamiento de un clasificador de √°rbol de decisi√≥n:\n","   - Se crea un clasificador de √°rbol de decisi√≥n (`DecisionTreeClassifier`) con criterio de entrop√≠a, una semilla aleatoria de 1 y una profundidad m√°xima de 1.\n","   - Se asigna el clasificador de √°rbol de decisi√≥n a la variable `clf`.\n","   - Se entrena el clasificador de √°rbol de decisi√≥n utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","3. Creaci√≥n y entrenamiento de un clasificador AdaBoost:\n","   - Se crea un clasificador AdaBoost (`AdaBoostClassifier`) utilizando el clasificador de √°rbol de decisi√≥n como estimador base.\n","   - Se configuran varios par√°metros, como el n√∫mero m√°ximo de clasificadores d√©biles en el ensemble (`n_estimators`), el algoritmo utilizado para el boosting (`algorithm`), etc.\n","   - Se entrena el clasificador AdaBoost utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","4. Evaluaci√≥n de los clasificadores:\n","   - Se imprime la precisi√≥n del clasificador de √°rbol de decisi√≥n utilizando los datos de prueba (`X_test` y `y_test`).\n","   - Se imprime la precisi√≥n del clasificador AdaBoost utilizando los datos de prueba (`X_test` y `y_test`).\n","\n","En resumen, este c√≥digo crea y entrena un clasificador de √°rbol de decisi√≥n y un clasificador AdaBoost, y luego eval√∫a su rendimiento en el conjunto de prueba. El clasificador AdaBoost utiliza m√∫ltiples clasificadores d√©biles (√°rboles de decisi√≥n en este caso) para mejorar su precisi√≥n al asignar pesos a las muestras y adaptarse a los casos dif√≠ciles de clasificar."],"metadata":{"id":"3a5dtXX4pxdT"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"BShkyRokpr1K"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","tree = DecisionTreeClassifier(criterion='entropy',\n","                              random_state=1,\n","                              max_depth=1)\n","clf =tree\n","clf.fit(X_train,y_train)\n","\n","boost = AdaBoostClassifier(\n","    estimator=tree,                # Clasificador base utilizado para construir el ensemble\n","    n_estimators=500,              # N√∫mero m√°ximo de clasificadores d√©biles en el ensemble\n","    algorithm='SAMME.R',           # Algoritmo utilizado para el boosting (SAMME.R utiliza probabilidades de clase)\n","    random_state=42                # Semilla utilizada por el generador de n√∫meros aleatorios para reproducibilidad\n",")\n","\n","boost.fit(X_train, y_train)\n","\n","print(\"Teee stump Accuracy: %0.2f\" % clf.score(X_test, y_test))\n","print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVRbDamUGWed","executionInfo":{"status":"ok","timestamp":1686823883346,"user_tz":180,"elapsed":2953,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"3c5c8e5a-1921-4b73-fdc7-bf843aac4a30"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Teee stump Accuracy: 0.87\n","Test Accuracy: 0.97\n"]}]},{"cell_type":"markdown","source":["### problema 5 Gradient boosting"],"metadata":{"id":"BgJkGphEiHMd"}},{"cell_type":"markdown","source":["El `GradientBoostingClassifier` es un algoritmo de conjunto (ensemble) de clasificaci√≥n que utiliza la t√©cnica de boosting para construir un modelo fuerte a partir de m√∫ltiples modelos d√©biles, en este caso, √°rboles de decisi√≥n.\n","\n","A continuaci√≥n, se describen los par√°metros principales del `GradientBoostingClassifier`:\n","\n","- `loss`: especifica la funci√≥n de p√©rdida a optimizar durante el entrenamiento. Puede ser \"deviance\", que se refiere a la p√©rdida logar√≠tmica para la clasificaci√≥n binaria, o \"exponential\", que se utiliza para la clasificaci√≥n multiclase. El valor predeterminado es \"deviance\".\n","\n","- `learning_rate`: controla la contribuci√≥n de cada √°rbol al modelo final. Un valor m√°s peque√±o reduce la importancia de cada √°rbol y puede ayudar a prevenir el sobreajuste, pero tambi√©n puede requerir un n√∫mero mayor de √°rboles para lograr un rendimiento √≥ptimo. El valor predeterminado es 0.1.\n","\n","- `n_estimators`: especifica el n√∫mero de √°rboles de decisi√≥n utilizados en el ensemble. Cuanto mayor sea este n√∫mero, m√°s complejo ser√° el modelo y m√°s tiempo de entrenamiento requerir√°. Sin embargo, tambi√©n puede aumentar el riesgo de sobreajuste. El valor predeterminado es 100.\n","\n","- `subsample`: controla la fracci√≥n de muestras utilizadas para entrenar cada √°rbol. Un valor menor a 1.0 introduce aleatoriedad y puede ayudar a reducir la varianza y el sobreajuste. El valor predeterminado es 1.0.\n","\n","- `max_depth`: especifica la profundidad m√°xima de los √°rboles de decisi√≥n. Limitar la profundidad puede ayudar a prevenir el sobreajuste y acelerar el entrenamiento. El valor predeterminado es 3.\n","\n","- `min_samples_split`: el n√∫mero m√≠nimo de muestras requeridas para dividir un nodo interno. Valores m√°s altos previenen divisiones que generan ramas con pocas muestras y ayudan a prevenir el sobreajuste. El valor predeterminado es 2.\n","\n","- `random_state`: establece la semilla para la generaci√≥n de n√∫meros aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `...`: hay otros par√°metros adicionales que se pueden configurar, como `min_samples_leaf`, `max_features`, etc., que afectan el crecimiento y la generalizaci√≥n de los √°rboles de decisi√≥n.\n","\n","En resumen, el `GradientBoostingClassifier` construye un ensemble de √°rboles de decisi√≥n mediante el enfoque de boosting, donde cada √°rbol se entrena en los errores residuales de los √°rboles anteriores. Los par√°metros mencionados anteriormente permiten ajustar la complejidad y el rendimiento del modelo final."],"metadata":{"id":"KCLu_zyoqSDq"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n","Este c√≥digo utiliza el algoritmo `GradientBoostingClassifier` para realizar la clasificaci√≥n mediante el uso de un ensemble de √°rboles de decisi√≥n.\n","\n","Aqu√≠ est√° lo que hace cada parte del c√≥digo:\n","\n","1. Importaci√≥n de la biblioteca necesaria:\n","   - `GradientBoostingClassifier` de `sklearn.ensemble` para crear un clasificador Gradient Boosting.\n","\n","2. Creaci√≥n y entrenamiento de un clasificador Gradient Boosting:\n","   - Se crea un clasificador Gradient Boosting (`GradientBoostingClassifier`) con los par√°metros especificados.\n","   - Los par√°metros incluyen el n√∫mero de iteraciones de boosting (`n_estimators`), la tasa de aprendizaje (`learning_rate`), la profundidad m√°xima de los √°rboles de decisi√≥n individuales (`max_depth`), y una semilla aleatoria para la reproducibilidad (`random_state`).\n","   - El clasificador Gradient Boosting se entrena utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","3. Evaluaci√≥n del clasificador Gradient Boosting:\n","   - Se imprime la precisi√≥n del clasificador Gradient Boosting utilizando los datos de prueba (`X_test` y `y_test`).\n","\n","En resumen, este c√≥digo crea y entrena un clasificador Gradient Boosting utilizando √°rboles de decisi√≥n como clasificadores d√©biles. El n√∫mero de iteraciones de boosting, la tasa de aprendizaje y la profundidad m√°xima de los √°rboles se configuran para ajustar el rendimiento del modelo. Luego, se eval√∫a la precisi√≥n del clasificador Gradient Boosting en el conjunto de prueba."],"metadata":{"id":"H5NUMsfoqWLT"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"FLRW_uwCqeb6"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Crear un clasificador Gradient Boosting\n","boost = GradientBoostingClassifier(\n","    n_estimators=50,          # N√∫mero de iteraciones de boosting para construir el ensemble\n","    learning_rate=0.1,        # Tasa de aprendizaje que controla la contribuci√≥n de cada clasificador d√©bil\n","    max_depth=5,              # Profundidad m√°xima de los √°rboles de decisi√≥n individuales\n","    random_state=42           # Semilla utilizada por el generador de n√∫meros aleatorios para reproducibilidad\n",")\n","\n","boost.fit(X_train, y_train)\n","\n","print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxwVHNv-G9vO","executionInfo":{"status":"ok","timestamp":1686823759759,"user_tz":180,"elapsed":741,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"3a1e2632-f0e2-4042-880d-6ff4931f3e08"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.94\n"]}]},{"cell_type":"markdown","source":["### problema 6 random forest üå≥"],"metadata":{"id":"hDFsIKmBf-AP"}},{"cell_type":"markdown","source":["#### ventajas y desventajas"],"metadata":{"id":"_X8PWdttq8Bq"}},{"cell_type":"markdown","source":["Random Forest: Algoritmo\n","- Cada √°rbol se construye utilizando el siguiente algoritmo:\n","- Sea $N$ el n√∫mero de casos de entrenamiento, y $M$ el n√∫mero de variables en el clasificador.\n","- Sabemos el n√∫mero $m$ de variables de entrada que se utilizar√°n para determinar la decisi√≥n en un nodo del √°rbol; $m$ deber√≠a ser mucho menor que $M$.\n","- Elija un conjunto de entrenamiento para este √°rbol eligiendo $n$ veces con reemplazo de todos los $N$ casos de entrenamiento disponibles (es decir, tome una muestra bootstrap).\n","- Use el resto de los casos para estimar el error del √°rbol, prediciendo sus clases.\n","- Para cada nodo del √°rbol, elija aleatoriamente $m$ variables en las que basar la decisi√≥n en ese nodo. Calcule la mejor divisi√≥n basada en estas $m$ variables en el conjunto de entrenamiento usando el √≠ndice de Gini.\n","- Cada √°rbol est√° completamente desarrollado y no podado.\n","- Para la predicci√≥n, se aplica el √°rbol a la muestra y se le asigna la etiqueta del nodo hoja donde termina.\n","- Este procedimiento se repite en todos los √°rboles del conjunto, y el voto mayoritario de todos los √°rboles se informa como predicci√≥n dela random forest."],"metadata":{"id":"IxkFqXiYf7_2"}},{"cell_type":"markdown","source":["**Ventajas de Random Forest:**\n","\n","1. Precisi√≥n: Es uno de los algoritmos de aprendizaje con menor tasa de error, lo que significa que produce clasificadores altamente precisos para muchos conjuntos de datos.\n","\n","2. Eficiencia en grandes bases de datos: Puede manejar eficientemente conjuntos de datos grandes sin comprometer el rendimiento.\n","\n","3. Manejo de variables de entrada: Puede manejar miles de variables de entrada sin necesidad de eliminar variables, lo que lo hace adecuado para conjuntos de datos con alta dimensionalidad.\n","\n","4. Importancia de variables: Proporciona estimaciones de qu√© variables son importantes en la clasificaci√≥n, lo que ayuda a comprender la relevancia de cada variable en el modelo.\n","\n","5. Estimaci√≥n interna del error de generalizaci√≥n: Genera una estimaci√≥n imparcial del error de generalizaci√≥n a medida que se construye el bosque, lo que ayuda a evaluar la capacidad predictiva del modelo.\n","\n","6. Manejo de datos faltantes: Tiene m√©todos efectivos para manejar datos faltantes y mantiene la precisi√≥n incluso cuando falta una gran parte de los datos.\n","\n","7. Equilibrio en conjuntos de datos no balanceados: Tiene m√©todos para equilibrar el error en conjuntos de datos donde las clases no est√°n balanceadas, lo que mejora el rendimiento en este tipo de situaciones.\n","\n","8. Almacenamiento y reutilizaci√≥n: Los bosques generados se pueden guardar y utilizar en futuros conjuntos de datos, lo que facilita la reutilizaci√≥n del modelo.\n","\n","9. An√°lisis de variables y proximidades: Proporciona informaci√≥n sobre la importancia de las variables y la relaci√≥n entre ellas, adem√°s de calcular las proximidades entre pares de casos, lo que permite realizar tareas como agrupamiento y detecci√≥n de valores at√≠picos.\n","\n","10. Extensi√≥n a datos no etiquetados: Las capacidades del Random Forest se pueden extender a datos no etiquetados, lo que permite realizar tareas de agrupamiento no supervisado, an√°lisis de vistas de datos y detecci√≥n de valores at√≠picos.\n","\n","**Desventajas de Random Forest:**\n","\n","1. Sobreajuste: Puede ocurrir sobreajuste en tareas de clasificaci√≥n/regresi√≥n con datos ruidosos, lo que puede afectar el rendimiento del modelo.\n","\n","2. Interpretaci√≥n compleja: A diferencia de los √°rboles de decisi√≥n individuales, la clasificaci√≥n realizada por Random Forest es dif√≠cil de interpretar por humanos, debido a la combinaci√≥n de m√∫ltiples √°rboles.\n","\n","3. Sesgo hacia variables categ√≥ricas: En presencia de variables categ√≥ricas con diferente n√∫mero de niveles, Random Forest puede mostrar un sesgo hacia los atributos con m√°s niveles, lo que puede afectar la fiabilidad del score de importancia de la variable.\n","\n","4. Correlaci√≥n entre grupos de atributos: Si los datos contienen grupos de atributos correlacionados con similar relevancia para el rendimiento, los grupos m√°s peque√±os pueden estar favorecidos sobre los grupos m√°s grandes, lo que puede afectar la interpretaci√≥n del modelo.\n","\n","‚ú®**resumen**‚ú®\n","\n","Ventajas de Random Forest:\n","- Alta precisi√≥n y bajo error en la clasificaci√≥n.\n","- Eficiente en el manejo de grandes bases de datos.\n","- Capacidad para manejar miles de variables de entrada sin eliminar caracter√≠sticas.\n","- Proporciona informaci√≥n sobre la importancia de las variables.\n","- Estimaci√≥n interna del error de generalizaci√≥n durante la construcci√≥n del modelo.\n","- Buen manejo de datos faltantes.\n","- Capacidad para equilibrar el error en conjuntos de datos no balanceados.\n","- Almacenamiento y reutilizaci√≥n del modelo.\n","- An√°lisis de variables y proximidades para agrupamiento y detecci√≥n de valores at√≠picos.\n","- Extensi√≥n a datos no etiquetados para tareas de agrupamiento y an√°lisis de datos.\n","\n","Desventajas de Random Forest:\n","- Posible sobreajuste en datos ruidosos.\n","- Dificultad en la interpretaci√≥n humana de la clasificaci√≥n realizada.\n","- Sesgo hacia variables categ√≥ricas con m√°s niveles.\n","- Impacto de la correlaci√≥n entre grupos de atributos en la interpretaci√≥n del modelo.\n","\n","\n","‚ú®**resumenen textito**‚ú®\n","\n","Random Forest es un algoritmo de aprendizaje ampliamente utilizado en Machine Learning debido a su alta precisi√≥n y capacidad para manejar grandes conjuntos de datos con m√∫ltiples variables. Destaca por su habilidad para identificar las caracter√≠sticas m√°s relevantes en la clasificaci√≥n, lo que brinda informaci√≥n valiosa para la toma de decisiones. Aunque su interpretaci√≥n puede resultar compleja y puede presentar ciertos desaf√≠os en presencia de datos ruidosos, Random Forest se posiciona como una herramienta confiable y eficiente en diversas aplicaciones de aprendizaje autom√°tico. En resumen, es una t√©cnica valiosa que combina precisi√≥n, escalabilidad y capacidad de selecci√≥n de caracter√≠sticas, lo que la convierte en una elecci√≥n frecuente en el campo del Machine Learning."],"metadata":{"id":"SLkW6-ncglDm"}},{"cell_type":"markdown","source":["#### aca te dice que hace el algoritmo"],"metadata":{"id":"Bt5FgXbGrInB"}},{"cell_type":"markdown","source":["El `GradientBoostingClassifier` es un algoritmo de aprendizaje supervisado basado en √°rboles de decisi√≥n que utiliza el m√©todo de boosting para construir un modelo predictivo robusto. A continuaci√≥n, se describen los par√°metros m√°s importantes del `GradientBoostingClassifier`:\n","\n","- `n_estimators`: Especifica el n√∫mero de etapas de boosting o iteraciones que se utilizar√°n para construir el ensemble. Cuanto mayor sea el n√∫mero de estimadores, m√°s complejo ser√° el modelo y mejor ser√° su capacidad de ajuste a los datos. Sin embargo, un n√∫mero muy alto de estimadores puede aumentar el tiempo de entrenamiento. El valor predeterminado es 100.\n","\n","- `learning_rate`: Controla la contribuci√≥n de cada √°rbol al modelo final. Es una tasa de aprendizaje que reduce la importancia de cada √°rbol, evitando as√≠ el sobreajuste. Un learning rate m√°s bajo requerir√° m√°s estimadores para alcanzar el mismo rendimiento que uno m√°s alto. El valor predeterminado es 0.1.\n","\n","- `max_depth`: Especifica la profundidad m√°xima de cada √°rbol de decisi√≥n individual en el ensemble. Limitar la profundidad puede prevenir el sobreajuste y mejorar la generalizaci√≥n del modelo. Si no se especifica, los √°rboles se expandir√°n hasta que todas las hojas sean puras o contengan el n√∫mero m√≠nimo de muestras requerido para una nueva divisi√≥n. No hay un valor predeterminado espec√≠fico para `max_depth`, ya que puede variar seg√∫n la naturaleza del problema y los datos.\n","\n","- `subsample`: Especifica la fracci√≥n de muestras utilizadas para ajustar cada √°rbol. Un valor menor a 1.0 introduce aleatoriedad y ayuda a reducir la varianza, lo que puede prevenir el sobreajuste. El valor predeterminado es 1.0, lo que significa que se utilizan todas las muestras.\n","\n","- `random_state`: Establece la semilla para la generaci√≥n de n√∫meros aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `loss`: Especifica la funci√≥n de p√©rdida a optimizar durante el entrenamiento. Puede ser \"deviance\", que se refiere a la p√©rdida logar√≠tmica para la clasificaci√≥n binaria, o \"exponential\", que se utiliza para la clasificaci√≥n multiclase. El valor predeterminado es \"deviance\".\n","\n","- `...`: Hay otros par√°metros adicionales que se pueden configurar, como `min_samples_split`, `min_samples_leaf`, `max_features`, etc., que afectan el crecimiento y la generalizaci√≥n de los √°rboles de decisi√≥n.\n","\n","En resumen, el `GradientBoostingClassifier` utiliza la t√©cnica de boosting para construir un ensemble de √°rboles de decisi√≥n. Los par√°metros mencionados anteriormente permiten ajustar la complejidad y el rendimiento del modelo final, controlando el n√∫mero de estimadores, la tasa de aprendizaje, la profundidad de los √°rboles y otros aspectos del entrenamiento."],"metadata":{"id":"OWlZlHsZq5V6"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n"],"metadata":{"id":"gZMdrSJxrakK"}},{"cell_type":"markdown","source":["Este c√≥digo crea un clasificador Gradient Boosting utilizando el `GradientBoostingClassifier` de scikit-learn. A continuaci√≥n, se describen los pasos que realiza:\n","\n","1. Importaci√≥n de la biblioteca necesaria:\n","   - Se importa la clase `GradientBoostingClassifier` del m√≥dulo `sklearn.ensemble`.\n","\n","2. Creaci√≥n del clasificador Gradient Boosting:\n","   - Se crea una instancia de `GradientBoostingClassifier` y se asigna a la variable `boost`.\n","   - Se especifican varios par√°metros para configurar el comportamiento del clasificador:\n","     - `n_estimators=50`: Indica el n√∫mero de iteraciones de boosting que se utilizar√°n para construir el ensemble. En este caso, se usar√°n 50 estimadores.\n","     - `learning_rate=0.1`: Controla la contribuci√≥n de cada clasificador d√©bil en el ensemble. Una tasa de aprendizaje m√°s baja significa una contribuci√≥n m√°s peque√±a de cada clasificador d√©bil.\n","     - `max_depth=5`: Especifica la profundidad m√°xima de los √°rboles de decisi√≥n individuales en el ensemble. En este caso, los √°rboles tendr√°n una profundidad m√°xima de 5.\n","     - `random_state=42`: Establece la semilla utilizada por el generador de n√∫meros aleatorios para garantizar la reproducibilidad de los resultados.\n","\n","3. Entrenamiento del clasificador Gradient Boosting:\n","   - Se entrena el clasificador Gradient Boosting utilizando los datos de entrenamiento `X_train` y `y_train` mediante el m√©todo `fit()`.\n","\n","4. Evaluaci√≥n del rendimiento del clasificador Gradient Boosting:\n","   - Se eval√∫a la precisi√≥n del clasificador Gradient Boosting en los datos de prueba `X_test` y `y_test` utilizando el m√©todo `score()`.\n","   - La precisi√≥n se imprime en la salida utilizando la funci√≥n `print()`.\n","\n","En resumen, el c√≥digo crea un clasificador Gradient Boosting con 50 estimadores, una tasa de aprendizaje de 0.1 y √°rboles de decisi√≥n con una profundidad m√°xima de 5. Luego, se entrena el clasificador y se eval√∫a su precisi√≥n en un conjunto de datos de prueba."],"metadata":{"id":"FD7GJn01rghS"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"Nox3xbg4rhjC"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# Crear un clasificador Random Forest\n","forest = RandomForestClassifier(\n","    n_estimators=20,           # N√∫mero de √°rboles en el bosque\n","    criterion='entropy',       # Criterio utilizado para medir la calidad de una partici√≥n ('gini' o 'entropy')\n","    max_depth=None,            # Profundidad m√°xima de los √°rboles (None significa que los √°rboles se expanden hasta que todas las hojas sean puras o contengan min_samples_split muestras)\n","    max_features='sqrt',       # N√∫mero de caracter√≠sticas a considerar al buscar la mejor partici√≥n ('sqrt' utiliza la ra√≠z cuadrada del n√∫mero total de caracter√≠sticas)\n","    min_samples_leaf=2,        # N√∫mero m√≠nimo de muestras requeridas para ser una hoja en un √°rbol\n","    min_samples_split=2,       # N√∫mero m√≠nimo de muestras requeridas para dividir un nodo interno\n","    random_state=42            # Semilla utilizada por el generador de n√∫meros aleatorios para reproducibilidad\n",")\n","\n","forest.fit(X_train, y_train)\n","\n","print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzd5Mmi6Jd_E","executionInfo":{"status":"ok","timestamp":1686823718645,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"d9e756fa-3494-4e3e-a09e-638cbf7ce018"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.93\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"j7fAzIDtJ8L9"},"execution_count":null,"outputs":[]}]}