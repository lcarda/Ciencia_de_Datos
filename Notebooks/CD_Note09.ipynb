{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["riv0vZp5ig3t"],"toc_visible":true,"authorship_tag":"ABX9TyObyEyq+halQXKcCcSrNTV7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["La descomposición en sesgo y varianza es un concepto utilizado en el aprendizaje automático para analizar el rendimiento de un modelo y comprender las fuentes de error.\n","\n","Sesgo (Bias): El sesgo se refiere a la capacidad de un modelo para realizar predicciones precisas en promedio. Un modelo con un sesgo alto tiende a simplificar demasiado los datos y puede perder detalles importantes, lo que resulta en un rendimiento deficiente en el conjunto de datos de entrenamiento y prueba. Esto se conoce como \"underfitting\". Un modelo con un sesgo bajo, por otro lado, tiene la capacidad de capturar patrones complejos en los datos y puede ajustarse bien tanto al conjunto de entrenamiento como al conjunto de prueba.\n","\n","Varianza (Variance): La varianza se refiere a la sensibilidad de un modelo a las fluctuaciones en los datos de entrenamiento. Un modelo con alta varianza se ajusta demasiado a los datos de entrenamiento y tiene dificultades para generalizar a nuevos datos, lo que resulta en un rendimiento deficiente en el conjunto de prueba. Esto se conoce como \"overfitting\". Por otro lado, un modelo con baja varianza tiene una capacidad limitada para capturar la complejidad de los datos y puede perder patrones importantes, lo que también puede llevar a un rendimiento deficiente en el conjunto de prueba.\n","\n","El objetivo es encontrar un equilibrio entre el sesgo y la varianza para obtener un modelo que se ajuste bien a los datos de entrenamiento y generalize bien a nuevos datos. Esto se conoce como el compromiso sesgo-varianza. Al comprender la descomposición en sesgo y varianza, se puede tomar decisiones más informadas sobre la elección de modelos y técnicas de regularización para mejorar el rendimiento predictivo."],"metadata":{"id":"EU_A7Z8peV-H"}},{"cell_type":"markdown","source":["- **Underfitting**: los errores de entrenamiento y testeo son ambos grandes. Sesgo alto.\n","- **Overfitting**: error al entrenar es pequeño, pero al testear aumenta. Varianza alta.\n","- Si el espacio de hipótesis que se estudia es grande hay mas tendencia a sobreajustar."],"metadata":{"id":"nEpek_Czehe1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PRFEiJ3FNZG9"}},{"cell_type":"code","source":["!pip install -U mlxtend"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":627},"id":"as3DzhoJmqbr","executionInfo":{"status":"ok","timestamp":1686824995226,"user_tz":180,"elapsed":6397,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"b2d0e87e-d202-42e3-fdad-c2719602ca10"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mlxtend in /usr/local/lib/python3.10/dist-packages (0.14.0)\n","Collecting mlxtend\n","  Downloading mlxtend-0.22.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.10.1)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.22.4)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.5.3)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.2)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (3.7.1)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from mlxtend) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mlxtend) (67.7.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->mlxtend) (2022.7.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n","Installing collected packages: mlxtend\n","  Attempting uninstall: mlxtend\n","    Found existing installation: mlxtend 0.14.0\n","    Uninstalling mlxtend-0.14.0:\n","      Successfully uninstalled mlxtend-0.14.0\n","Successfully installed mlxtend-0.22.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["mlxtend"]}}},"metadata":{}}]},{"cell_type":"markdown","source":["### problema 2  Votacion de la mayoria"],"metadata":{"id":"ElrzgK_VnBXD"}},{"cell_type":"markdown","source":["`EnsembleVoteClassifier` es un clasificador de votación de conjunto implementado en la biblioteca `mlxtend`. Combina múltiples clasificadores base para realizar predicciones mediante votación.\n","\n","Cuando se crea un objeto `EnsembleVoteClassifier`, se deben proporcionar dos argumentos principales:\n","- `clfs`: una lista de clasificadores base que se utilizarán en el conjunto de votación.\n","- `weights`: una lista de pesos asignados a cada clasificador base, que determina la importancia relativa de cada clasificador en la votación.\n","\n","Durante la fase de entrenamiento, cada clasificador base se ajusta (entrena) con los datos de entrenamiento proporcionados. Durante la fase de predicción, los clasificadores base hacen sus predicciones individualmente y luego se realiza una votación para determinar la clase final de la muestra.\n","\n","El tipo de votación se define mediante el parámetro `voting` en la creación del objeto `EnsembleVoteClassifier`. Puede ser \"hard\" (votación dura) o \"soft\" (votación suave):\n","- En la votación dura, la clase más común predicha por los clasificadores base se elige como la predicción final.\n","- En la votación suave, se asigna una puntuación de confianza a cada clase para cada clasificador base, y las predicciones se ponderan por estas puntuaciones antes de realizar la votación final.\n","\n","Al utilizar `EnsembleVoteClassifier`, se busca aprovechar las fortalezas de diferentes clasificadores base y mejorar el rendimiento general del conjunto a través de la votación. Puede ser especialmente útil cuando los clasificadores base tienen enfoques y características diferentes.\n","\n","En el código que proporcionaste, `EnsembleVoteClassifier` se utiliza para combinar tres clasificadores de árbol de decisión con diferentes profundidades máximas en un clasificador de votación de conjunto, donde cada clasificador base tiene un peso igual (1) en la votación."],"metadata":{"id":"VC1SBz0Unvvz"}},{"cell_type":"markdown","source":["#####explicacion del codigo"],"metadata":{"id":"h4JgPJmmnxQr"}},{"cell_type":"markdown","source":["\n","Este código realiza un ejemplo de clasificación utilizando el conjunto de datos Iris. Aquí está lo que hace cada parte del código:\n","\n","Importación de las bibliotecas necesarias:\n","\n","model_selection de sklearn para realizar selección de modelos y evaluación.\n","DecisionTreeClassifier de sklearn.tree para crear un clasificador de árbol de decisión.\n","train_test_split de sklearn.model_selection para dividir los datos en conjuntos de entrenamiento y prueba.\n","datasets de sklearn para cargar el conjunto de datos Iris.\n","EnsembleVoteClassifier de mlxtend.classifier para crear un clasificador de votación de conjunto.\n","Carga del conjunto de datos Iris:\n","\n","Utiliza la función load_iris() de datasets para cargar el conjunto de datos Iris.\n","Divide los datos en características (X) y etiquetas (y), tomando las columnas 0 y 3 del conjunto de datos original.\n","División de los datos en conjuntos de entrenamiento, validación y prueba:\n","\n","Utiliza la función train_test_split para dividir los datos en conjuntos de entrenamiento y prueba, con un tamaño de prueba del 25% y una semilla aleatoria de 1.\n","A partir del conjunto de entrenamiento, se vuelve a dividir en conjuntos de entrenamiento y validación, con un tamaño de validación del 25% y una semilla aleatoria de 1.\n","Creación de clasificadores:\n","\n","Se definen tres clasificadores de árbol de decisión con diferentes profundidades máximas (1, 3 y la predeterminada).\n","Se crea un clasificador de votación de conjunto (EnsembleVoteClassifier) que combina los tres clasificadores anteriores con pesos iguales.\n","Entrenamiento y evaluación de los clasificadores:\n","\n","Se itera sobre los clasificadores y se realiza el entrenamiento en el conjunto de entrenamiento.\n","Se evalúa el rendimiento de cada clasificador utilizando el conjunto de validación y se imprime la precisión de la clasificación.\n","Evaluación del clasificador de votación de conjunto:\n","\n","Se evalúa el rendimiento del clasificador de votación de conjunto utilizando el conjunto de prueba y se imprime la precisión de la clasificación.\n","En resumen, este código muestra cómo utilizar los clasificadores de árbol de decisión y el clasificador de votación de conjunto en el conjunto de datos Iris, realizando la evaluación tanto en un conjunto de validación como en un conjunto de prueba."],"metadata":{"id":"VPRoxAGZnIj_"}},{"cell_type":"markdown","source":["####Codigo"],"metadata":{"id":"3eUULJ4Fn2EL"}},{"cell_type":"code","source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from mlxtend.classifier import EnsembleVoteClassifier\n","\n","iris = datasets.load_iris()\n","X, y = iris.data[:, [0, 3]], iris.target\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","clf1 = DecisionTreeClassifier(random_state=1)\n","clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n","clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n","eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n","\n","labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n","for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n","\n","    clf.fit(X_train, y_train)\n","    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n","\n","print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_WW7IgbmW8r","executionInfo":{"status":"ok","timestamp":1686825018741,"user_tz":180,"elapsed":4799,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"8aeb3716-04e8-4e3c-afc1-289e510cb30b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 84 28 38\n","Validation Accuracy: 0.86 [Classifier 1]\n","Validation Accuracy: 0.82 [Classifier 2]\n","Validation Accuracy: 0.93 [Classifier 3]\n","Validation Accuracy: 0.93 [Ensemble]\n","Test Accuracy: 0.95\n"]}]},{"cell_type":"markdown","source":["#### load breast cancer"],"metadata":{"id":"BOYfAfV8r8DJ"}},{"cell_type":"code","source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from mlxtend.classifier import EnsembleVoteClassifier\n","\n","# Cargar el conjunto de datos de cáncer de mama\n","cancer = datasets.load_breast_cancer()\n","X, y = cancer.data, cancer.target\n","\n","# Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","# Dividir los datos de entrenamiento en conjuntos de entrenamiento y validación\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","# Crear los clasificadores base\n","clf1 = DecisionTreeClassifier(random_state=1)\n","clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n","clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n","\n","# Crear el clasificador de conjunto (Ensemble)\n","eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n","\n","labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n","for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n","    clf.fit(X_train, y_train)\n","    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n","\n","print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42DVVfUxsDbS","executionInfo":{"status":"ok","timestamp":1686826401726,"user_tz":180,"elapsed":361,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"5d320f7a-31d2-4c91-e38d-70f652fd378f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 319 107 143\n","Validation Accuracy: 0.93 [Classifier 1]\n","Validation Accuracy: 0.92 [Classifier 2]\n","Validation Accuracy: 0.92 [Classifier 3]\n","Validation Accuracy: 0.92 [Ensemble]\n","Test Accuracy: 0.91\n"]}]},{"cell_type":"markdown","source":["### problema 3 Bagging: Bootstrap Aggregating\n","\n","\n"],"metadata":{"id":"PP6C_Eu9oPtk"}},{"cell_type":"markdown","source":["El BaggingClassifier es otro método de conjunto (ensemble) implementado en la biblioteca scikit-learn. Bagging significa \"ensacado\" y es una técnica que se utiliza para mejorar la estabilidad y precisión de los modelos de aprendizaje automático.\n","\n","En términos simples, el BaggingClassifier crea múltiples instancias de un clasificador base y entrena cada instancia en una muestra aleatoria con reemplazo del conjunto de datos de entrenamiento original. Luego, combina las predicciones de cada clasificador base mediante votación (promediando en caso de regresión) para obtener una predicción final.\n","\n","Al crear un objeto BaggingClassifier, se deben proporcionar los siguientes argumentos principales:\n","\n","base_estimator: el clasificador base que se utilizará en cada instancia.\n","n_estimators: el número de clasificadores base (instancias) que se crearán.\n","max_samples: el número de muestras seleccionadas aleatoriamente con reemplazo para entrenar cada clasificador base.\n","max_features: el número de características seleccionadas aleatoriamente en cada división de los datos durante la construcción del clasificador base.\n","Además, el BaggingClassifier ofrece opciones adicionales, como el uso de muestras ponderadas, paralelización para entrenamiento rápido y la opción de permitir o no muestras fuera de la bolsa para estimaciones.\n","\n","El objetivo principal del BaggingClassifier es reducir la varianza y el sobreajuste al promediar las predicciones de múltiples clasificadores base entrenados en muestras aleatorias. Esto puede mejorar el rendimiento general del modelo y hacerlo más robusto.\n","\n","En resumen, el BaggingClassifier crea un conjunto de clasificadores base, los entrena en muestras aleatorias y combina sus predicciones mediante votación. Esto ayuda a mejorar la estabilidad y precisión del modelo al reducir la varianza y el sobreajuste."],"metadata":{"id":"wDNQByfxpRgr"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n","\n","\n","Este código utiliza el conjunto de datos Breast Cancer para realizar la clasificación mediante el uso de un clasificador de árbol de decisión y un clasificador Bagging.\n","\n","Aquí está lo que hace cada parte del código:\n","\n","Importación de las bibliotecas necesarias:\n","\n","model_selection de sklearn para realizar selección de modelos y evaluación.\n","DecisionTreeClassifier de sklearn.tree para crear un clasificador de árbol de decisión.\n","train_test_split de sklearn.model_selection para dividir los datos en conjuntos de entrenamiento y prueba.\n","datasets de sklearn para cargar conjuntos de datos.\n","BaggingClassifier de sklearn.ensemble para crear un clasificador Bagging.\n","Carga del conjunto de datos Breast Cancer:\n","\n","Utiliza la función load_breast_cancer() de datasets para cargar el conjunto de datos Breast Cancer.\n","Divide los datos en características (X) y etiquetas (y).\n","División de los datos en conjuntos de entrenamiento, validación y prueba:\n","\n","Utiliza la función train_test_split para dividir los datos en conjuntos de entrenamiento y prueba, con un tamaño de prueba del 25% y una semilla aleatoria de 1.\n","A partir del conjunto de entrenamiento, se vuelve a dividir en conjuntos de entrenamiento y validación, con un tamaño de validación del 25% y una semilla aleatoria de 1.\n","Creación y entrenamiento de un clasificador de árbol de decisión:\n","\n","Se crea un clasificador de árbol de decisión (DecisionTreeClassifier) con criterio de entropía y una semilla aleatoria de 1.\n","Se asigna el clasificador de árbol de decisión a la variable clf.\n","Se entrena el clasificador de árbol de decisión utilizando los datos de entrenamiento (X_train y y_train).\n","Creación y entrenamiento de un clasificador Bagging:\n","\n","Se crea un clasificador Bagging (BaggingClassifier) utilizando el clasificador de árbol de decisión como estimador base.\n","Se configuran varios parámetros, como el número de estimadores en el ensemble (n_estimators), el uso del cálculo del puntaje fuera de la bolsa (oob_score), el muestreo con reemplazo (bootstrap), etc.\n","Se entrena el clasificador Bagging utilizando los datos de entrenamiento (X_train y y_train).\n","Evaluación de los clasificadores:\n","\n","Se imprime la precisión del clasificador de árbol de decisión utilizando los datos de prueba (X_test y y_test).\n","Se imprime el puntaje fuera de la bolsa del clasificador Bagging (bag.oob_score_).\n","Se imprime la precisión del clasificador Bagging utilizando los datos de prueba (X_test y y_test).\n","En resumen, este código carga el conjunto de datos Breast Cancer, entrena un clasificador de árbol de decisión y un clasificador Bagging, y luego evalúa el rendimiento de ambos clasificadores en el conjunto de prueba."],"metadata":{"id":"_wNKXPORo85D"}},{"cell_type":"markdown","source":["####codigo"],"metadata":{"id":"NYS8bpOEo7Oa"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYwrw6HW_B90","executionInfo":{"status":"ok","timestamp":1686823714717,"user_tz":180,"elapsed":4525,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"a6cb4a64-9f86-485e-f6b1-bd5b1d3f105a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train/Valid/Test sizes: 319 107 143\n","tree Accuracy: 0.94\n","OOB Accuracy: 0.95\n","Test Accuracy: 0.97\n"]}],"source":["from sklearn import model_selection\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from sklearn.ensemble import BaggingClassifier\n","\n","from sklearn.datasets import load_breast_cancer\n","cancer = load_breast_cancer()\n","X = cancer.data\n","y = cancer.target\n","target_names = cancer.target_names\n","\n","\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.25, random_state=1)\n","\n","X_train, X_val, y_train, y_val = \\\n","    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n","\n","print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n","\n","tree = DecisionTreeClassifier(criterion='entropy',\n","                              random_state=1,\n","                              max_depth=None)\n","\n","clf = tree\n","clf.fit(X_train, y_train)\n","\n","bag = BaggingClassifier(\n","    estimator=tree,                    # Clasificador base utilizado para construir el ensemble\n","    n_estimators=500,                  # Número de estimadores en el ensemble\n","    oob_score=True,                     # Indicador para calcular el score fuera de la bolsa (OOB score)\n","    bootstrap=True,                     # Indicador para realizar muestreo con reemplazo (bootstrap)\n","    bootstrap_features=False,           # Indicador para realizar muestreo de características con reemplazo\n","    n_jobs=1,                           # Número de trabajos en paralelo para ajustar los estimadores base\n","    random_state=42                     # Semilla utilizada por el generador de números aleatorios para reproducibilidad\n",")\n","\n","bag.fit(X_train, y_train)\n","\n","print('tree Accuracy: %0.2f' % clf.score(X_test, y_test))\n","print('OOB Accuracy: %0.2f' % bag.oob_score_)\n","print('Test Accuracy: %0.2f' % bag.score(X_test,y_test))"]},{"cell_type":"markdown","source":["### problema 4 adaptative boosting"],"metadata":{"id":"riv0vZp5ig3t"}},{"cell_type":"markdown","source":["El `AdaBoostClassifier` es un algoritmo de conjunto (ensemble) de clasificación implementado en la biblioteca `scikit-learn`. El nombre \"AdaBoost\" proviene de \"Adaptive Boosting\" (\"impulso adaptable\"), que describe su enfoque de mejorar iterativamente el rendimiento del clasificador.\n","\n","El algoritmo `AdaBoostClassifier` combina múltiples clasificadores débiles (también conocidos como estimadores base) en un clasificador fuerte. Cada clasificador débil se entrena en un conjunto de datos ponderado, donde las muestras mal clasificadas por los clasificadores anteriores se les da más peso para el siguiente clasificador. En resumen, el algoritmo se adapta y se centra en las muestras difíciles de clasificar, mejorando así el rendimiento general.\n","\n","A continuación, se describen los parámetros principales del `AdaBoostClassifier`:\n","\n","- `base_estimator`: especifica el clasificador débil utilizado en el ensemble. Puede ser cualquier clasificador de `scikit-learn` compatible con clasificación binaria, como `DecisionTreeClassifier`. De forma predeterminada, se utiliza un árbol de decisión de profundidad 1 (`DecisionTreeClassifier(max_depth=1)`).\n","\n","- `n_estimators`: especifica el número de clasificadores débiles (estimadores) en el ensemble. Cuanto mayor sea este número, más complejo será el ensemble. Sin embargo, también puede aumentar el riesgo de sobreajuste. El valor predeterminado es `50`.\n","\n","- `learning_rate`: controla la contribución de cada clasificador débil en el ensemble. Un valor más pequeño disminuye la importancia de cada clasificador, lo que puede ayudar a prevenir el sobreajuste. El valor predeterminado es `1.0`.\n","\n","- `algorithm`: especifica el algoritmo utilizado para el cálculo de los pesos de muestra. Puede ser \"SAMME\" (Stagewise Additive Modeling using a Multiclass Exponential loss function) o \"SAMME.R\" (SAMME con la tasa de probabilidad). \"SAMME.R\" generalmente se recomienda ya que puede manejar estimadores con valores de probabilidad continuos. El valor predeterminado es \"SAMME.R\".\n","\n","- `random_state`: establece la semilla para la generación de números aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `...`: hay otros parámetros adicionales que se pueden configurar, como `max_depth`, `min_samples_split`, etc., que afectan al clasificador débil utilizado.\n","\n","En resumen, el `AdaBoostClassifier` utiliza clasificadores débiles para formar un ensemble y mejorar iterativamente su rendimiento al centrarse en las muestras difíciles de clasificar. Los parámetros mencionados anteriormente permiten ajustar la complejidad del ensemble y controlar su capacidad de generalización."],"metadata":{"id":"B1CDTvoQpqLK"}},{"cell_type":"markdown","source":["#### explicacion del codigo"],"metadata":{"id":"ZTmFsS2WpsBb"}},{"cell_type":"markdown","source":["Este código utiliza el algoritmo AdaBoostClassifier para realizar la clasificación mediante el uso de un clasificador de árbol de decisión como estimador base.\n","\n","Aquí está lo que hace cada parte del código:\n","\n","1. Importación de las bibliotecas necesarias:\n","   - `AdaBoostClassifier` de `sklearn.ensemble` para crear un clasificador AdaBoost.\n","   - `DecisionTreeClassifier` de `sklearn.tree` para crear un clasificador de árbol de decisión.\n","\n","2. Creación y entrenamiento de un clasificador de árbol de decisión:\n","   - Se crea un clasificador de árbol de decisión (`DecisionTreeClassifier`) con criterio de entropía, una semilla aleatoria de 1 y una profundidad máxima de 1.\n","   - Se asigna el clasificador de árbol de decisión a la variable `clf`.\n","   - Se entrena el clasificador de árbol de decisión utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","3. Creación y entrenamiento de un clasificador AdaBoost:\n","   - Se crea un clasificador AdaBoost (`AdaBoostClassifier`) utilizando el clasificador de árbol de decisión como estimador base.\n","   - Se configuran varios parámetros, como el número máximo de clasificadores débiles en el ensemble (`n_estimators`), el algoritmo utilizado para el boosting (`algorithm`), etc.\n","   - Se entrena el clasificador AdaBoost utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","4. Evaluación de los clasificadores:\n","   - Se imprime la precisión del clasificador de árbol de decisión utilizando los datos de prueba (`X_test` y `y_test`).\n","   - Se imprime la precisión del clasificador AdaBoost utilizando los datos de prueba (`X_test` y `y_test`).\n","\n","En resumen, este código crea y entrena un clasificador de árbol de decisión y un clasificador AdaBoost, y luego evalúa su rendimiento en el conjunto de prueba. El clasificador AdaBoost utiliza múltiples clasificadores débiles (árboles de decisión en este caso) para mejorar su precisión al asignar pesos a las muestras y adaptarse a los casos difíciles de clasificar."],"metadata":{"id":"3a5dtXX4pxdT"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"BShkyRokpr1K"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","tree = DecisionTreeClassifier(criterion='entropy',\n","                              random_state=1,\n","                              max_depth=1)\n","clf =tree\n","clf.fit(X_train,y_train)\n","\n","boost = AdaBoostClassifier(\n","    estimator=tree,                # Clasificador base utilizado para construir el ensemble\n","    n_estimators=500,              # Número máximo de clasificadores débiles en el ensemble\n","    algorithm='SAMME.R',           # Algoritmo utilizado para el boosting (SAMME.R utiliza probabilidades de clase)\n","    random_state=42                # Semilla utilizada por el generador de números aleatorios para reproducibilidad\n",")\n","\n","boost.fit(X_train, y_train)\n","\n","print(\"Teee stump Accuracy: %0.2f\" % clf.score(X_test, y_test))\n","print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVRbDamUGWed","executionInfo":{"status":"ok","timestamp":1686823883346,"user_tz":180,"elapsed":2953,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"3c5c8e5a-1921-4b73-fdc7-bf843aac4a30"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Teee stump Accuracy: 0.87\n","Test Accuracy: 0.97\n"]}]},{"cell_type":"markdown","source":["### problema 5 Gradient boosting"],"metadata":{"id":"BgJkGphEiHMd"}},{"cell_type":"markdown","source":["El `GradientBoostingClassifier` es un algoritmo de conjunto (ensemble) de clasificación que utiliza la técnica de boosting para construir un modelo fuerte a partir de múltiples modelos débiles, en este caso, árboles de decisión.\n","\n","A continuación, se describen los parámetros principales del `GradientBoostingClassifier`:\n","\n","- `loss`: especifica la función de pérdida a optimizar durante el entrenamiento. Puede ser \"deviance\", que se refiere a la pérdida logarítmica para la clasificación binaria, o \"exponential\", que se utiliza para la clasificación multiclase. El valor predeterminado es \"deviance\".\n","\n","- `learning_rate`: controla la contribución de cada árbol al modelo final. Un valor más pequeño reduce la importancia de cada árbol y puede ayudar a prevenir el sobreajuste, pero también puede requerir un número mayor de árboles para lograr un rendimiento óptimo. El valor predeterminado es 0.1.\n","\n","- `n_estimators`: especifica el número de árboles de decisión utilizados en el ensemble. Cuanto mayor sea este número, más complejo será el modelo y más tiempo de entrenamiento requerirá. Sin embargo, también puede aumentar el riesgo de sobreajuste. El valor predeterminado es 100.\n","\n","- `subsample`: controla la fracción de muestras utilizadas para entrenar cada árbol. Un valor menor a 1.0 introduce aleatoriedad y puede ayudar a reducir la varianza y el sobreajuste. El valor predeterminado es 1.0.\n","\n","- `max_depth`: especifica la profundidad máxima de los árboles de decisión. Limitar la profundidad puede ayudar a prevenir el sobreajuste y acelerar el entrenamiento. El valor predeterminado es 3.\n","\n","- `min_samples_split`: el número mínimo de muestras requeridas para dividir un nodo interno. Valores más altos previenen divisiones que generan ramas con pocas muestras y ayudan a prevenir el sobreajuste. El valor predeterminado es 2.\n","\n","- `random_state`: establece la semilla para la generación de números aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `...`: hay otros parámetros adicionales que se pueden configurar, como `min_samples_leaf`, `max_features`, etc., que afectan el crecimiento y la generalización de los árboles de decisión.\n","\n","En resumen, el `GradientBoostingClassifier` construye un ensemble de árboles de decisión mediante el enfoque de boosting, donde cada árbol se entrena en los errores residuales de los árboles anteriores. Los parámetros mencionados anteriormente permiten ajustar la complejidad y el rendimiento del modelo final."],"metadata":{"id":"KCLu_zyoqSDq"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n","Este código utiliza el algoritmo `GradientBoostingClassifier` para realizar la clasificación mediante el uso de un ensemble de árboles de decisión.\n","\n","Aquí está lo que hace cada parte del código:\n","\n","1. Importación de la biblioteca necesaria:\n","   - `GradientBoostingClassifier` de `sklearn.ensemble` para crear un clasificador Gradient Boosting.\n","\n","2. Creación y entrenamiento de un clasificador Gradient Boosting:\n","   - Se crea un clasificador Gradient Boosting (`GradientBoostingClassifier`) con los parámetros especificados.\n","   - Los parámetros incluyen el número de iteraciones de boosting (`n_estimators`), la tasa de aprendizaje (`learning_rate`), la profundidad máxima de los árboles de decisión individuales (`max_depth`), y una semilla aleatoria para la reproducibilidad (`random_state`).\n","   - El clasificador Gradient Boosting se entrena utilizando los datos de entrenamiento (`X_train` y `y_train`).\n","\n","3. Evaluación del clasificador Gradient Boosting:\n","   - Se imprime la precisión del clasificador Gradient Boosting utilizando los datos de prueba (`X_test` y `y_test`).\n","\n","En resumen, este código crea y entrena un clasificador Gradient Boosting utilizando árboles de decisión como clasificadores débiles. El número de iteraciones de boosting, la tasa de aprendizaje y la profundidad máxima de los árboles se configuran para ajustar el rendimiento del modelo. Luego, se evalúa la precisión del clasificador Gradient Boosting en el conjunto de prueba."],"metadata":{"id":"H5NUMsfoqWLT"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"FLRW_uwCqeb6"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Crear un clasificador Gradient Boosting\n","boost = GradientBoostingClassifier(\n","    n_estimators=50,          # Número de iteraciones de boosting para construir el ensemble\n","    learning_rate=0.1,        # Tasa de aprendizaje que controla la contribución de cada clasificador débil\n","    max_depth=5,              # Profundidad máxima de los árboles de decisión individuales\n","    random_state=42           # Semilla utilizada por el generador de números aleatorios para reproducibilidad\n",")\n","\n","boost.fit(X_train, y_train)\n","\n","print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxwVHNv-G9vO","executionInfo":{"status":"ok","timestamp":1686823759759,"user_tz":180,"elapsed":741,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"3a1e2632-f0e2-4042-880d-6ff4931f3e08"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.94\n"]}]},{"cell_type":"markdown","source":["### problema 6 random forest 🌳"],"metadata":{"id":"hDFsIKmBf-AP"}},{"cell_type":"markdown","source":["#### ventajas y desventajas"],"metadata":{"id":"_X8PWdttq8Bq"}},{"cell_type":"markdown","source":["Random Forest: Algoritmo\n","- Cada árbol se construye utilizando el siguiente algoritmo:\n","- Sea $N$ el número de casos de entrenamiento, y $M$ el número de variables en el clasificador.\n","- Sabemos el número $m$ de variables de entrada que se utilizarán para determinar la decisión en un nodo del árbol; $m$ debería ser mucho menor que $M$.\n","- Elija un conjunto de entrenamiento para este árbol eligiendo $n$ veces con reemplazo de todos los $N$ casos de entrenamiento disponibles (es decir, tome una muestra bootstrap).\n","- Use el resto de los casos para estimar el error del árbol, prediciendo sus clases.\n","- Para cada nodo del árbol, elija aleatoriamente $m$ variables en las que basar la decisión en ese nodo. Calcule la mejor división basada en estas $m$ variables en el conjunto de entrenamiento usando el índice de Gini.\n","- Cada árbol está completamente desarrollado y no podado.\n","- Para la predicción, se aplica el árbol a la muestra y se le asigna la etiqueta del nodo hoja donde termina.\n","- Este procedimiento se repite en todos los árboles del conjunto, y el voto mayoritario de todos los árboles se informa como predicción dela random forest."],"metadata":{"id":"IxkFqXiYf7_2"}},{"cell_type":"markdown","source":["**Ventajas de Random Forest:**\n","\n","1. Precisión: Es uno de los algoritmos de aprendizaje con menor tasa de error, lo que significa que produce clasificadores altamente precisos para muchos conjuntos de datos.\n","\n","2. Eficiencia en grandes bases de datos: Puede manejar eficientemente conjuntos de datos grandes sin comprometer el rendimiento.\n","\n","3. Manejo de variables de entrada: Puede manejar miles de variables de entrada sin necesidad de eliminar variables, lo que lo hace adecuado para conjuntos de datos con alta dimensionalidad.\n","\n","4. Importancia de variables: Proporciona estimaciones de qué variables son importantes en la clasificación, lo que ayuda a comprender la relevancia de cada variable en el modelo.\n","\n","5. Estimación interna del error de generalización: Genera una estimación imparcial del error de generalización a medida que se construye el bosque, lo que ayuda a evaluar la capacidad predictiva del modelo.\n","\n","6. Manejo de datos faltantes: Tiene métodos efectivos para manejar datos faltantes y mantiene la precisión incluso cuando falta una gran parte de los datos.\n","\n","7. Equilibrio en conjuntos de datos no balanceados: Tiene métodos para equilibrar el error en conjuntos de datos donde las clases no están balanceadas, lo que mejora el rendimiento en este tipo de situaciones.\n","\n","8. Almacenamiento y reutilización: Los bosques generados se pueden guardar y utilizar en futuros conjuntos de datos, lo que facilita la reutilización del modelo.\n","\n","9. Análisis de variables y proximidades: Proporciona información sobre la importancia de las variables y la relación entre ellas, además de calcular las proximidades entre pares de casos, lo que permite realizar tareas como agrupamiento y detección de valores atípicos.\n","\n","10. Extensión a datos no etiquetados: Las capacidades del Random Forest se pueden extender a datos no etiquetados, lo que permite realizar tareas de agrupamiento no supervisado, análisis de vistas de datos y detección de valores atípicos.\n","\n","**Desventajas de Random Forest:**\n","\n","1. Sobreajuste: Puede ocurrir sobreajuste en tareas de clasificación/regresión con datos ruidosos, lo que puede afectar el rendimiento del modelo.\n","\n","2. Interpretación compleja: A diferencia de los árboles de decisión individuales, la clasificación realizada por Random Forest es difícil de interpretar por humanos, debido a la combinación de múltiples árboles.\n","\n","3. Sesgo hacia variables categóricas: En presencia de variables categóricas con diferente número de niveles, Random Forest puede mostrar un sesgo hacia los atributos con más niveles, lo que puede afectar la fiabilidad del score de importancia de la variable.\n","\n","4. Correlación entre grupos de atributos: Si los datos contienen grupos de atributos correlacionados con similar relevancia para el rendimiento, los grupos más pequeños pueden estar favorecidos sobre los grupos más grandes, lo que puede afectar la interpretación del modelo.\n","\n","✨**resumen**✨\n","\n","Ventajas de Random Forest:\n","- Alta precisión y bajo error en la clasificación.\n","- Eficiente en el manejo de grandes bases de datos.\n","- Capacidad para manejar miles de variables de entrada sin eliminar características.\n","- Proporciona información sobre la importancia de las variables.\n","- Estimación interna del error de generalización durante la construcción del modelo.\n","- Buen manejo de datos faltantes.\n","- Capacidad para equilibrar el error en conjuntos de datos no balanceados.\n","- Almacenamiento y reutilización del modelo.\n","- Análisis de variables y proximidades para agrupamiento y detección de valores atípicos.\n","- Extensión a datos no etiquetados para tareas de agrupamiento y análisis de datos.\n","\n","Desventajas de Random Forest:\n","- Posible sobreajuste en datos ruidosos.\n","- Dificultad en la interpretación humana de la clasificación realizada.\n","- Sesgo hacia variables categóricas con más niveles.\n","- Impacto de la correlación entre grupos de atributos en la interpretación del modelo.\n","\n","\n","✨**resumenen textito**✨\n","\n","Random Forest es un algoritmo de aprendizaje ampliamente utilizado en Machine Learning debido a su alta precisión y capacidad para manejar grandes conjuntos de datos con múltiples variables. Destaca por su habilidad para identificar las características más relevantes en la clasificación, lo que brinda información valiosa para la toma de decisiones. Aunque su interpretación puede resultar compleja y puede presentar ciertos desafíos en presencia de datos ruidosos, Random Forest se posiciona como una herramienta confiable y eficiente en diversas aplicaciones de aprendizaje automático. En resumen, es una técnica valiosa que combina precisión, escalabilidad y capacidad de selección de características, lo que la convierte en una elección frecuente en el campo del Machine Learning."],"metadata":{"id":"SLkW6-ncglDm"}},{"cell_type":"markdown","source":["#### aca te dice que hace el algoritmo"],"metadata":{"id":"Bt5FgXbGrInB"}},{"cell_type":"markdown","source":["El `GradientBoostingClassifier` es un algoritmo de aprendizaje supervisado basado en árboles de decisión que utiliza el método de boosting para construir un modelo predictivo robusto. A continuación, se describen los parámetros más importantes del `GradientBoostingClassifier`:\n","\n","- `n_estimators`: Especifica el número de etapas de boosting o iteraciones que se utilizarán para construir el ensemble. Cuanto mayor sea el número de estimadores, más complejo será el modelo y mejor será su capacidad de ajuste a los datos. Sin embargo, un número muy alto de estimadores puede aumentar el tiempo de entrenamiento. El valor predeterminado es 100.\n","\n","- `learning_rate`: Controla la contribución de cada árbol al modelo final. Es una tasa de aprendizaje que reduce la importancia de cada árbol, evitando así el sobreajuste. Un learning rate más bajo requerirá más estimadores para alcanzar el mismo rendimiento que uno más alto. El valor predeterminado es 0.1.\n","\n","- `max_depth`: Especifica la profundidad máxima de cada árbol de decisión individual en el ensemble. Limitar la profundidad puede prevenir el sobreajuste y mejorar la generalización del modelo. Si no se especifica, los árboles se expandirán hasta que todas las hojas sean puras o contengan el número mínimo de muestras requerido para una nueva división. No hay un valor predeterminado específico para `max_depth`, ya que puede variar según la naturaleza del problema y los datos.\n","\n","- `subsample`: Especifica la fracción de muestras utilizadas para ajustar cada árbol. Un valor menor a 1.0 introduce aleatoriedad y ayuda a reducir la varianza, lo que puede prevenir el sobreajuste. El valor predeterminado es 1.0, lo que significa que se utilizan todas las muestras.\n","\n","- `random_state`: Establece la semilla para la generación de números aleatorios. Esto asegura la reproducibilidad del ensemble.\n","\n","- `loss`: Especifica la función de pérdida a optimizar durante el entrenamiento. Puede ser \"deviance\", que se refiere a la pérdida logarítmica para la clasificación binaria, o \"exponential\", que se utiliza para la clasificación multiclase. El valor predeterminado es \"deviance\".\n","\n","- `...`: Hay otros parámetros adicionales que se pueden configurar, como `min_samples_split`, `min_samples_leaf`, `max_features`, etc., que afectan el crecimiento y la generalización de los árboles de decisión.\n","\n","En resumen, el `GradientBoostingClassifier` utiliza la técnica de boosting para construir un ensemble de árboles de decisión. Los parámetros mencionados anteriormente permiten ajustar la complejidad y el rendimiento del modelo final, controlando el número de estimadores, la tasa de aprendizaje, la profundidad de los árboles y otros aspectos del entrenamiento."],"metadata":{"id":"OWlZlHsZq5V6"}},{"cell_type":"markdown","source":["#### explicacion del codigo\n"],"metadata":{"id":"gZMdrSJxrakK"}},{"cell_type":"markdown","source":["Este código crea un clasificador Gradient Boosting utilizando el `GradientBoostingClassifier` de scikit-learn. A continuación, se describen los pasos que realiza:\n","\n","1. Importación de la biblioteca necesaria:\n","   - Se importa la clase `GradientBoostingClassifier` del módulo `sklearn.ensemble`.\n","\n","2. Creación del clasificador Gradient Boosting:\n","   - Se crea una instancia de `GradientBoostingClassifier` y se asigna a la variable `boost`.\n","   - Se especifican varios parámetros para configurar el comportamiento del clasificador:\n","     - `n_estimators=50`: Indica el número de iteraciones de boosting que se utilizarán para construir el ensemble. En este caso, se usarán 50 estimadores.\n","     - `learning_rate=0.1`: Controla la contribución de cada clasificador débil en el ensemble. Una tasa de aprendizaje más baja significa una contribución más pequeña de cada clasificador débil.\n","     - `max_depth=5`: Especifica la profundidad máxima de los árboles de decisión individuales en el ensemble. En este caso, los árboles tendrán una profundidad máxima de 5.\n","     - `random_state=42`: Establece la semilla utilizada por el generador de números aleatorios para garantizar la reproducibilidad de los resultados.\n","\n","3. Entrenamiento del clasificador Gradient Boosting:\n","   - Se entrena el clasificador Gradient Boosting utilizando los datos de entrenamiento `X_train` y `y_train` mediante el método `fit()`.\n","\n","4. Evaluación del rendimiento del clasificador Gradient Boosting:\n","   - Se evalúa la precisión del clasificador Gradient Boosting en los datos de prueba `X_test` y `y_test` utilizando el método `score()`.\n","   - La precisión se imprime en la salida utilizando la función `print()`.\n","\n","En resumen, el código crea un clasificador Gradient Boosting con 50 estimadores, una tasa de aprendizaje de 0.1 y árboles de decisión con una profundidad máxima de 5. Luego, se entrena el clasificador y se evalúa su precisión en un conjunto de datos de prueba."],"metadata":{"id":"FD7GJn01rghS"}},{"cell_type":"markdown","source":["#### codigo"],"metadata":{"id":"Nox3xbg4rhjC"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# Crear un clasificador Random Forest\n","forest = RandomForestClassifier(\n","    n_estimators=20,           # Número de árboles en el bosque\n","    criterion='entropy',       # Criterio utilizado para medir la calidad de una partición ('gini' o 'entropy')\n","    max_depth=None,            # Profundidad máxima de los árboles (None significa que los árboles se expanden hasta que todas las hojas sean puras o contengan min_samples_split muestras)\n","    max_features='sqrt',       # Número de características a considerar al buscar la mejor partición ('sqrt' utiliza la raíz cuadrada del número total de características)\n","    min_samples_leaf=2,        # Número mínimo de muestras requeridas para ser una hoja en un árbol\n","    min_samples_split=2,       # Número mínimo de muestras requeridas para dividir un nodo interno\n","    random_state=42            # Semilla utilizada por el generador de números aleatorios para reproducibilidad\n",")\n","\n","forest.fit(X_train, y_train)\n","\n","print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzd5Mmi6Jd_E","executionInfo":{"status":"ok","timestamp":1686823718645,"user_tz":180,"elapsed":8,"user":{"displayName":"Lucas Dario Cardacci","userId":"14722131315120574206"}},"outputId":"d9e756fa-3494-4e3e-a09e-638cbf7ce018"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.93\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"j7fAzIDtJ8L9"},"execution_count":null,"outputs":[]}]}